{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Chatbot engine with Oracle Database 23ai on Google Cloud\n",
    "The main purpose of this workshop is to teach you how you can implement a **RAG (Retrieval Augmented Generation)** chatbot using vector similarity search and Generative AI / LLMs.\n",
    "\n",
    "We will guide you through the process of loading and parsing a FAQ-like text file, integrating it with an Oracle 23ai database, and employing the Google Cloud Platform to order it and run the Python glue code and Generative AI services needed for the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the working environment in Google Cloud\n",
    "\n",
    "The prerequisites needed to run this notebook are described [here](https://github.com/WSPluta/multicloud).\n",
    "\n",
    "In a nutsehll, we need: \n",
    "- a Google Cloud tenancy\n",
    "- a working multicloud Oracle Database 23ai\n",
    "- a Linux (Ubuntu) virtual machine\n",
    "- access to Google's Gemini generative AI service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will use Visual Studio Code (VSCode) to connect to our Ubuntu VM on Google Cloud and run all the configuration steps, edit, and run our Jupyter Notebook.\n",
    "\n",
    "Please use VSCode's Remote Explorer function to connect to your remote VM. If you don't know how to do that, please see [this tutorial first](https://code.visualstudio.com/docs/remote/ssh).\n",
    "\n",
    "Next, install `pyenv` on the remote machine. This is our way to quickly and neatly manage multiple Python versions on the same machine. For this tutorial, we will use Python 3.12. If you are more comfortable with other ways to install the desired Python version on a Linux box, feel free to use it instead.\n",
    "\n",
    "Open the terminal pane in VSCode and run the following commands (for the latest version of this procedure, see [the official pyenv page here](https://github.com/pyenv/pyenv-installer)):\n",
    "\n",
    "```\n",
    "sudo apt install -y make build-essential libssl-dev zlib1g-dev libbz2-dev \\\n",
    "libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev \\\n",
    "xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git\n",
    "\n",
    "curl https://pyenv.run | bash\n",
    "\n",
    "echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.profile\n",
    "echo 'command -v pyenv >/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.profile\n",
    "echo 'eval \"$(pyenv init -)\"' >> ~/.profile\n",
    "\n",
    "echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc\n",
    "echo 'command -v pyenv >/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc\n",
    "echo 'eval \"$(pyenv init -)\"' >> ~/.bashrc\n",
    "\n",
    "exec \"$SHELL\"\n",
    "```\n",
    "\n",
    "Now it should be easy to get Python 3.12 on your machine:\n",
    "\n",
    "```\n",
    "pyenv install 3.12\n",
    "\n",
    "```\n",
    "\n",
    "Once done, create a new folder called `vectors` in your VSCode explorer window and copy this notebook and the `faq.txt` file inside it.\n",
    "\n",
    "Back in the terminal pane, `cd` to the `vectors` folder and make Python 3.12 the active kernel for it:\n",
    "\n",
    "```\n",
    "cd vectors\n",
    "pyenv local 3.12\n",
    "```\n",
    "\n",
    "Last step in this phase is installing the Python libraries for accessing the Oracle Database and sentence transformers (to convert strings to vectors):\n",
    "\n",
    "```\n",
    "pip install oracledb\n",
    "pip install sentence-transformers\n",
    "```\n",
    "\n",
    "Open the `vectors.ipynb` file (this file, really) in VSCode and continue reading while executing the code cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vector embeddings\n",
    "\n",
    "This section guides you through the code required to load text files from a local folder, split them into segments, and then create embeddings and ingest them into the Oracle 23ai vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts\n",
    "\n",
    "- What is a Vector?\n",
    "\n",
    "    A vector is like a fingerprint for information. Just like every person’s fingerprint is unique and gives us lots of details about them, a vector in AI is a unique set of numbers representing the important semantic features of a piece of information, like a block of text, an image, a sound, or a video.\n",
    "\n",
    "- What is Similarity Search and Outlier Search?\n",
    "\n",
    "    A similarity search helps you find semantically similar things, like Google or Bing does. But imagine being able to do that in the database, with text, audio, image, and video files and the full power of SQL and PL/SQL at your disposal. An outlier search does the opposite: it retrieves the most dissimilar results.\n",
    "\n",
    "- What is a LLM?\n",
    "\n",
    "    LLMs, or Large Language Models, are AI algorithms that use deep learning techniques and large data sets to understand, summarize, generate, and predict new content. Oracle AI Vector Search works well with any Large Language Model [LLM] and vector embedding model.\n",
    "\n",
    "- What is RAG?\n",
    "\n",
    "    Retrieval Augmented Generation (RAG) is a technique that enhances LLMs by integrating Similarity Search. This enables use cases such as a corporate chatbot responding with private company knowledge to make sure it’s giving answers that are up-to-date and tailored to your business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Loading the text sources from a file.\n",
    "\n",
    "Let's work with our source files, which contain the knowledge we want to use as the foundation for our chatbot's responses.\n",
    "FAQs are loaded from a file, encoded, and stored. For this example, we will use a properly formatted plain text FAQ, following the pattern:\n",
    "\n",
    "```\n",
    "What is Oracle Cloud Free Tier?\n",
    "\n",
    "Oracle Cloud Free Tier allows you to sign up for an Oracle Cloud account which provides a number of Always Free services and a Free Trial with US$300 of free credit to use on all eligible Oracle Cloud Infrastructure services for up to 30 days. The Always Free services are available for an unlimited period of time. The Free Trial services may be used until your US$300 of free credits are consumed or the 30 days has expired, whichever comes first.\n",
    "\n",
    "=====\n",
    "\n",
    "Who should use Oracle Cloud Free Tier?\n",
    "\n",
    "Oracle Cloud Free Tier services are for everyone. Whether you’re a developer building and testing applications, a startup founder creating new systems with the intention of scaling later, an enterprise looking to test things before moving to cloud, a student wanting to learn, or an academic developing curriculum in the cloud, Oracle Cloud Free Tier enables you to learn, explore, build and test for free.\n",
    "\n",
    "=====\n",
    "\n",
    "Why do I need to provide credit or debit card information when I sign up for Oracle Cloud Free Tier?\n",
    "\n",
    "To provide free Oracle Cloud accounts to our valued customers, we need to ensure that you are who you say you are. We use your contact information and credit/debit card information for account setup and identity verification. Oracle may periodically check the validity of your card, resulting in a temporary “authorization” hold. These holds are removed by your bank, typically within three to five days, and do not result in actual charges to your account.\n",
    "\n",
    "=====\n",
    "\n",
    "```\n",
    "So, we have the question, an empty line, the answer, and then a separator denoted by =====. For this simple example, we load the whole thing into memory. For a small FAQ file, there is no need for a proper vector database; however, if your knowledge base grows, you will want one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will open all the .txt files in a specified folder, read them, split the content using the ======== separator. It will then put all the resulting chunks in an array.\n",
    "\n",
    "The array is stored inside a dictionary with the file name used as the key. This will be useful later if many other FAQ files are available inside the folder, helping to differentiate between the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def loadFAQs(directory_path):\n",
    "   faqs = {}\n",
    "\n",
    "   for filename in os.listdir(directory_path):\n",
    "      if filename.endswith(\".txt\"):  # assuming FAQs are in .txt files\n",
    "         file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "         with open(file_path) as f:\n",
    "            raw_faq = f.read()\n",
    "\n",
    "         filename_without_ext = os.path.splitext(filename)[0]  # remove .txt extension\n",
    "         faqs[filename_without_ext] = [text.strip() for text in raw_faq.split('=====')]\n",
    "\n",
    "   return faqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faqs = loadFAQs('.')\n",
    "faqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in preparing the source data is to arrange the above dictionary in a way that is easy to ingest in the vector database. Enter this code into a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'faq | Who are you and what can you do?\\n\\nI am DORA, the Digital ORacle Assistant, a digital assistant working for Oracle EMEA. I can answer questions about Oracle Cloud (OCI) and especially about the Free Trial and Always Free programs.',\n",
       "  'path': 'faq'},\n",
       " {'text': 'faq | What is Oracle Cloud Free Tier?\\n\\nOracle Cloud Free Tier allows you to sign up for an Oracle Cloud account which provides a number of Always Free services and a Free Trial with US$300 of free credit to use on all eligible Oracle Cloud Infrastructure services for up to 30 days. The Always Free services are available for an unlimited period of time. The Free Trial services may be used until your US$300 of free credits are consumed or the 30 days has expired, whichever comes first.',\n",
       "  'path': 'faq'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [{'text': filename + ' | ' + section, 'path': filename} for filename, sections in faqs.items() for section in sections]\n",
    "\n",
    "# Sample the resulting data\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Loading the FAQ chunks into the vector database\n",
    "\n",
    "Make sure you downloaded the wallet file from the database's cloud console page, as described in the prerequisites. Upload it to the remote VM in VSCode and place it in the folder above the `vectors` folder. (Or anywhere else, but be sure to update the path to the wallet twice in the cell below.)\n",
    "\n",
    "Also, replace the username and password for the wallet and the database user in the cell too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "\n",
    "un = \"user\"\n",
    "pw = \"password\"\n",
    "dsn = 'doramulticloud_high'\n",
    "\n",
    "connection = oracledb.connect(\n",
    "    config_dir = '../wallet', \n",
    "    user=un, \n",
    "    password=pw, \n",
    "    dsn=dsn,\n",
    "    wallet_location = '../wallet',\n",
    "    wallet_password = 'password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the `faqs` table. We need a table inside our database to store our vectors and metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'faqs'\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # Create the table\n",
    "    create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id NUMBER PRIMARY KEY,\n",
    "            payload CLOB CHECK (payload IS JSON),\n",
    "            vector VECTOR\n",
    "        )\"\"\"\n",
    "    try:\n",
    "        cursor.execute(create_table_sql)\n",
    "    except oracledb.DatabaseError as e:\n",
    "        raise\n",
    "\n",
    "    connection.autocommit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the text chunks from the FAQ file, we need an encoder to handle the vectorization for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go through all our chunks (stored in the docs dictionary) and encode the text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:18<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "import array\n",
    "\n",
    "# Define a list to store the data\n",
    "data = [\n",
    "   {\"id\": idx, \"vector_source\": row['text'], \"payload\": row} \n",
    "   for idx, row in enumerate(docs)\n",
    "]\n",
    "\n",
    "# Collect all texts for batch encoding\n",
    "texts = [f\"{row['vector_source']}\" for row in data]\n",
    "\n",
    "# Encode all texts in a batch\n",
    "embeddings = encoder.encode(texts, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Assign the embeddings back to your data structure\n",
    "for row, embedding in zip(data, embeddings):\n",
    "   row['vector'] = array.array(\"f\", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, now we have a structure with all our chunks, including its context — the source file name, in this simple example — and the vector representation for each of them.\n",
    "\n",
    "Next step is about inserting the chunks + vectors in the database.\n",
    "\n",
    "Initially, we use a cursor object from the established database connection to execute a command that truncates the specified table. This operation ensures that all existing rows are removed, effectively resetting the table to an empty state and preparing it for fresh data insertion.\n",
    "\n",
    "Subsequently, the code prepares a list of tuples containing the new data. Each tuple includes an id, a JSON-encoded payload, and a vector.\n",
    "\n",
    "The `json.dumps` function is used to convert the payload into a JSON string format, ensuring that complex data structures are properly serialized for database storage.\n",
    "\n",
    "We then utilize the `cursor.executemany` method to insert all prepared tuples into the table in a single batch operation. This method is highly efficient for handling bulk inserts, significantly reducing the number of database transactions and improving performance. Finally, the `connection.commit` method is called to commit the transaction, ensuring that all changes are saved and made permanent in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # Truncate the table\n",
    "    cursor.execute(f\"truncate table {table_name}\")\n",
    "\n",
    "    prepared_data = [(row['id'], json.dumps(row['payload']), row['vector']) for row in data]\n",
    "\n",
    "    # Insert the data\n",
    "    cursor.executemany(\n",
    "        f\"\"\"INSERT INTO {table_name} (id, payload, vector)\n",
    "        VALUES (:1, :2, :3)\"\"\",\n",
    "        prepared_data\n",
    "    )\n",
    "\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to confirm, let's examine what is currently stored in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    # Define the query to select all rows from a table\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Print the rows\n",
    "    for row in rows[:5]:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our database contains all our knowledge base, so we are ready to proceed to the next step, basing it on users' questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector retrieval and Large Language Model generation\n",
    "\n",
    "This section guides you through the steps to integrate the vector database (Oracle Database 23ai in our case) and retrieve a list of text chunks that are close to the \"question\" in vector space. Then, we will use the most relevant text chunks to create an LLM prompt and ask the GenAI service to create a nicely worded response for us.\n",
    "\n",
    "This is a classical Retrieval-Augmented Generation (RAG) approach. The Retrieval-Augmented Generation architecture combines retrieval-based and generation-based methods to enhance natural language processing tasks. It consists of a retriever, which searches a knowledge base for relevant documents, and a generator, which uses these documents to produce informed responses. This dual approach improves accuracy and detail compared to models relying solely on pre-trained knowledge.\n",
    "\n",
    "The retriever finds the most pertinent documents by embedding both queries and documents into the same vector space. This ensures that the top-N relevant documents are selected to provide additional context for the generator. By doing so, the retriever enhances the quality of the generated text, particularly for queries needing specific or up-to-date information.\n",
    "\n",
    "The generator integrates the retrieved documents into its response generation. It may concatenate the query with the retrieved texts or use attention mechanisms to focus on relevant parts, producing coherent and contextually rich responses. This combination allows RAG to handle diverse tasks, making it a versatile tool in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the \"question\"\n",
    "\n",
    "In this step, we will take a text, supposedly the question the user is asking to the bot, and transform it into a vector. We will then feed this vector to the database, where it will be used to retrieve similar vectors and associated metadata, which are stored in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the SQL script used to retrieve the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 3\n",
    "\n",
    "sql = f\"\"\"select payload, vector_distance(vector, :vector, COSINE) as score\n",
    "from {table_name}\n",
    "order by score\n",
    "fetch approx first {topK} rows only\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given SQL query, `topK` represents the number of top results to retrieve. The query selects the payload column along with the cosine distance between the vector column in the specified table `table_name` and a provided vector parameter `:vector`, aliasing the distance calculation as `score`.\n",
    "\n",
    "By ordering the results by the calculated score and using fetch approx first `topK` rows only, the query efficiently retrieves only the top `topK` results based on their cosine similarity to the provided vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform this question into a vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is Always Free?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ the same encoder as in previous text chunks, generating a vector representation of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "  embedding = list(encoder.encode(question))\n",
    "  vector = array.array(\"f\", embedding)\n",
    "\n",
    "  results  = []\n",
    "\n",
    "  for (info, score, ) in cursor.execute(sql, vector=vector):\n",
    "      text_content = info.read()\n",
    "      results.append((score, json.loads(text_content)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL query is executed with the provided vector parameter, fetching relevant information from the database. For each result, the code retrieves the text content, stored in JSON format, and appends it to a list along with the calculated similarity score. This process iterates through all fetched results, accumulating them in the results list.\n",
    "\n",
    "If we print the results, we obtain something like the following. As requested, we have the \"score\" of each hit, which is essentially the distance in vector space between the question and the text chunk, as well as the metadata JSON embedded in each chunk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3420591950416565,\n",
      "  {'text': 'faq | What are Always Free services?\\n'\n",
      "           '\\n'\n",
      "           'Always Free services are part of Oracle Cloud Free Tier. Always '\n",
      "           'Free services are available for an unlimited time. Some '\n",
      "           'limitations apply. As new Always Free services become available, '\n",
      "           'you will automatically be able to use those as well.\\n'\n",
      "           '\\n'\n",
      "           'The following services are available as Always Free:\\n'\n",
      "           '\\n'\n",
      "           'AMD-based Compute\\n'\n",
      "           'Arm-based Ampere A1 Compute\\n'\n",
      "           'Block Volume\\n'\n",
      "           'Object Storage\\n'\n",
      "           'Archive Storage\\n'\n",
      "           'Flexible Load Balancer\\n'\n",
      "           'Flexible Network Load Balancer\\n'\n",
      "           'VPN Connect\\n'\n",
      "           'Autonomous Data Warehouse\\n'\n",
      "           'Autonomous Transaction Processing\\n'\n",
      "           'Autonomous JSON Database\\n'\n",
      "           'NoSQL Database (Phoenix Region only)\\n'\n",
      "           'APEX Application Development\\n'\n",
      "           'Resource Manager (Terraform)\\n'\n",
      "           'Monitoring\\n'\n",
      "           'Notifications\\n'\n",
      "           'Logging\\n'\n",
      "           'Application Performance Monitoring\\n'\n",
      "           'Service Connector Hub\\n'\n",
      "           'Vault\\n'\n",
      "           'Bastions\\n'\n",
      "           'Security Advisor\\n'\n",
      "           'Virtual Cloud Networks\\n'\n",
      "           'Site-to-Site VPN\\n'\n",
      "           'Content Management Starter Edition\\n'\n",
      "           'Email Delivery',\n",
      "   'path': 'faq'}),\n",
      " (0.4832669496536255,\n",
      "  {'text': 'faq | Are Always Free services available for paid accounts?\\n'\n",
      "           '\\n'\n",
      "           'Yes, for paid accounts using universal credit pricing.',\n",
      "   'path': 'faq'}),\n",
      " (0.4878004789352417,\n",
      "  {'text': 'faq | Could you elaborate on the concept of Always Free resources '\n",
      "           'in Oracle Cloud Infrastructure, and how can users leverage them '\n",
      "           'for various use cases while staying within the specified '\n",
      "           'limitations?\\n'\n",
      "           '\\n'\n",
      "           'Always Free resources in Oracle Cloud Infrastructure are services '\n",
      "           'and resources that can be used without incurring charges, subject '\n",
      "           'to certain usage limitations. Users can leverage these resources '\n",
      "           'for development, testing, small-scale applications, and learning '\n",
      "           'purposes, all while adhering to the restrictions outlined in the '\n",
      "           'terms and conditions.',\n",
      "   'path': 'faq'})]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pp(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LLM prompt\n",
    "\n",
    "In a Retrieval-Augmented Generation (RAG) application, the prompt given to a Large Language Model (LLM) is crucial for ensuring that the model generates accurate and contextually relevant responses. It effectively integrates retrieved information with the query, clarifies user intent, and frames the context in which the LLM operates. A well-crafted prompt enhances the relevance and accuracy of the generated text by providing clear instructions and integrating various aspects of the retrieved data. This is essential for optimizing performance, handling complex queries, and delivering precise and user-satisfactory outputs in real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sending anything to the LLM, we must ensure that our prompt does not exceed the maximum context length of the model. A smaller context length also ensures faster responses, usually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "import sys\n",
    "\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "\n",
    "\n",
    "tokenizer.model_max_length = sys.maxsize\n",
    "\n",
    "def truncate_string(string, max_tokens):\n",
    "    # Tokenize the text and count the tokens\n",
    "    tokens = tokenizer.encode(string, add_special_tokens=True) \n",
    "    # Truncate the tokens to a maximum length\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    # transform the tokens back to text\n",
    "    truncated_text = tokenizer.decode(truncated_tokens)\n",
    "    return truncated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code leverages the Hugging Face Transformers library to tokenize text using the `LlamaTokenizerFast` model. The tokenizer is initialized from the pre-trained `hf-internal-testing/llama-tokenizer` model, and its `model_max_length` attribute is set to `sys.maxsize` to handle extremely large inputs without length constraints.\n",
    "\n",
    "The `truncate_string` function takes a string and a maximum token count as inputs. It tokenizes the input string, truncates the tokenized sequence to the specified maximum length, and then decodes the truncated tokens back into a string. This function effectively shortens the text to a specified token limit while preserving its readable format, useful for tasks requiring length constraints on input text.\n",
    "\n",
    "We will truncate our chunks to 1000 tokens, to leave plenty of space for the rest of the prompt and the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform docs into a string array using the \"paylod\" key\n",
    "docs_as_one_string = \"\\n=========\\n\".join([doc[\"text\"] for doc in docs])\n",
    "docs_truncated = truncate_string(docs_as_one_string, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the prompt will include the retrieved top chunks, the question posed by the user, and the custom instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\\\n",
    "    Respond to PRECISELY to this question: \"{question}.\",  USING ONLY the following information and IGNORING ANY PREVIOUS KNOWLEDGE.\n",
    "    =====\n",
    "    Sources: {docs_truncated}\n",
    "    =====\n",
    "    Answer (Three paragraphs, maximum 50 words each, 90% spartan):\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Generative AI Service LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, log in to Google Cloud using the right credentials.\n",
    "\n",
    "```\n",
    "pip install --upgrade google-cloud-aiplatform\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, SafetySetting, Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the general parameters we want for our LLM calls. Feel free to adjust them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialize the model, an important part is the system prompt, as you can see below. Strongly instructing it to ignore previous knowledge (acquired during the initial training of the model) is very important if we want it to first look at our text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"directed-racer-440911-k0\", location=\"europe-central2\")\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-002\",\n",
    "    system_instruction=[\"\"\"\n",
    "    You are a helpful assistant named Oracle chatbot. \n",
    "    USE ONLY the sources below and ABSOLUTELY IGNORE any previous knowledge.\n",
    "    Use Markdown if appropriate.\n",
    "    Assume the customer is highly technical.     \n",
    "    Include code snippets and commands where necessary.\n",
    "    NEVER mention the sources, always respond as if you have that knowledge yourself. Do NOT provide warnings or disclaimers.          \n",
    "    \"\"\"]\n",
    ")\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's call the model with our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      text: \"Always Free services are a component of the Oracle Cloud Free Tier, offering unlimited access to specific services.  These include AMD-based and Arm-based compute instances, block and object storage, and archive storage.  Further Always Free services may be added automatically.\\n\\n\\nLimitations apply to Always Free services.  Unlike the Free Trial\\'s $300 credit,  Always Free services have no time limit. However, Always Free users don\\'t receive SLAs or Oracle Support.\\n\\n\\nCommunity forums offer support for all users.  To access Oracle Support and SLAs, upgrade to a paid account after consuming Free Trial credits or when the trial expires.\\n\"\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  avg_logprobs: -0.3260646877866803\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 1042\n",
       "  candidates_token_count: 132\n",
       "  total_token_count: 1174\n",
       "}\n",
       "model_version: \"gemini-1.5-flash-002\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = chat.send_message(\n",
    "    prompt,\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings\n",
    ")\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a nice answer, based on our small knowledge base, coming from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Always Free services are a component of the Oracle Cloud Free Tier, offering unlimited access to specific services.  These include AMD-based and '\n",
      " 'Arm-based compute instances, block and object storage, and archive storage.  Further Always Free services may be added automatically.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"Limitations apply to Always Free services.  Unlike the Free Trial's $300 credit,  Always Free services have no time limit. However, Always Free \"\n",
      " \"users don't receive SLAs or Oracle Support.\\n\"\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Community forums offer support for all users.  To access Oracle Support and SLAs, upgrade to a paid account after consuming Free Trial credits or '\n",
      " 'when the trial expires.\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(r.candidates[0].content.parts[0].text, width=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
